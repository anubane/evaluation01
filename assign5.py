###############################
#####   Anurag Banerjee   #####
#####       17071003      #####
#####     Assignment 5    #####
###############################

import numpy as np
import pandas as pd


class FileReader:
    """
    In this class we will read a given file and build a numpy ndarray for it
    """
    def __init__(self, path, cols):
        self.arr = np.empty((0, cols))
        try:
            self.handle = open(path, 'r')   # safely open given file
        except Exception as e:
            print("In file " + path + " some God forsaken error happened!")
            raise e

    def buildarray(self):
        for line in self.handle:
            fields = line[:len(line)-1].split()     # read each line of file but exclude the \n at end
            self.arr = np.append(self.arr, [fields], axis=0)    # build a numpy array
        return self.arr


"""def join():
    d = np.union1d(a[:, 0],b[:, 0]).reshape(-1,1)
    z = np.zeros((d.shape[0],2),dtype=int)
    c = np.hstack((d,z))
    mask = np.in1d(c[:, 0], b[:, 0])
    c[mask,1] = b[:, 1]
    mask = np.in1d(c[:, 0], a[:, 0])
    c[mask,2] = a[:, 1]
    print c"""


def getEvalTbl(qrelsDF, resultsDF):
    """
    This function performs an inner join on the two tables and generates a new pandas dataframe with only the required
    columns, viz., qid, did, rel, rank.
    :param qrelsDF: A pandas dataframe that has data regarding the relevance judgement
    :param resultsDF: A pandas dataframe that has data regarding the ranks and scores
    :return: the evaluation table
    """
    eval_tbl = (qrelsDF.drop(columns='hist')).merge(resultsDF.drop(columns=['hist', 'score', 'model']), left_on=['qid', 'did'], right_on=['qid', 'did'], how='inner')
    # eval_tbl = eval_tbl.groupby('qid').apply(lambda x: x.sort_values('rank'))
    return eval_tbl


def getDFs(qrelFile, qrelFields, resFile, resFields):
    """
    Use the FileReader class to get the numpy arrays from files and then convert them into pandas dataframes.
    :param qrelFile: Filename of the file containing the relevance judgement data
    :param qrelFields: number of fields in the relevance judgement data
    :param resFile: Filename of the file containing the results data (ranking, score)
    :param resFields: number of fields in the output data
    :return: dataframes for relevance judgement, output and a list of query ids
    """
    # Qrels have 4 fields: <Qid, unused historic field, doc-id, rel-info>
    obj = FileReader(qrelFile, qrelFields)
    qrels = obj.buildarray(4)
    qrelsDF = pd.DataFrame.from_records(qrels, columns=['qid', 'hist', 'did', 'rel'])
    qrelsDF[['rel']] = qrelsDF[['rel']].apply(pd.to_numeric)
    qids = qrelsDF.qid.unique()

    # 6 fields <Qid, unused historic field, doc-id, rank, score, model-name>
    obj = FileReader(resFile, resFields)
    results = obj.buildarray(6)
    resultsDF = pd.DataFrame.from_records(results, columns=['qid', 'hist', 'did', 'rank', 'score', 'model'])
    resultsDF[['rank']] = resultsDF[['rank']].apply(pd.to_numeric)
    return qrelsDF, resultsDF, qids


def getavgprec(eval_tbl, qid_rel_doc_count):
    """
    Calculate the average precision (natural) for each of the query topics.
    :param eval_tbl: the evaluation table generated by getEvalTbl()
    :param qid_rel_doc_count:   a dictionary holding relevant doc count for each query topic
    :return: a list of average precisions for each query topic
    """
    ap = []
    for qid, qid_eval_tbl in eval_tbl.groupby('qid'):
        # print('Data for qid = ' + qid + ': ')
        qid_eval_tbl = qid_eval_tbl.sort_values(by=['rank'], ascending=[True])
        qid_eval_tbl = qid_eval_tbl.reset_index(drop=True)
        # print(qid_eval_tbl.dtypes)
        cumu_rel = 0
        rprec = []
        for i, row in qid_eval_tbl.iterrows():
            cumu_rel += row['rel']
            rprec.append(cumu_rel / (i+1))
        qid_eval_tbl['rprec'] = rprec
        rprecDF = qid_eval_tbl['rprec'].loc[qid_eval_tbl['rel'] == 1]
        # average precision = sum of rprec of rel docs retrieved/total number of rel docs
        ap.append(rprecDF.sum() / qid_rel_doc_count[qid])
    return ap


def getmap(ap, topic_count):
    """
    Calculate the mean average precision score.
    :param ap: list of average precisions (natural) for each query
    :param topic_count: number of query topics 
    :return: the MAP score
    """
    return sum(ap) / topic_count


def main():
    """
    Pseudo starting point of the program.
    :return: None
    """
    (qrelsDF, resultsDF, qids) = getDFs("qrels.test", 4, "results.test", 6)
    qrelsRelDF = qrelsDF.loc[qrelsDF['rel'] == 1]
    qid_rel_doc_count = {}
    for qid, qid_rel_doc in qrelsRelDF.groupby('qid'):
        qid_rel_doc_count[qid] = qid_rel_doc.shape[0]
    # print(qid_rel_doc_count)

    eval_tbl = getEvalTbl(qrelsDF, resultsDF)
    # print(eval_tbl)

    ap = getavgprec(eval_tbl, qid_rel_doc_count)
    for i, qid in enumerate(qid_rel_doc_count.keys()):
        print("Average rprec for " + str(qid) + " is: " + str(ap[i]))
    map_score = getmap(ap, len(qid_rel_doc_count))
    print("Mean average precision for all topics: " + str(map_score))


if __name__ == '__main__':
    main()

# EOF
